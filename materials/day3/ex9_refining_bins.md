# Manually refining bins

### Objectives

* Prepare input files for `VizBin`
* Project a *t-SNE* using `VizBin` and examine bin clusters
* Refine bins by removing incorrectly assigned contigs
* *Optional:* Creating new `VizBin` profiles with different fragment lengths
* *Optional:* Scripts for processing data with `ESOMana`
* Assigning taxonomy to the refined bins

---

### Prepare input files for *VizBin*

[**VizBin**](http://claczny.github.io/VizBin/) is a handy, GUI-based tool for creating ordinations of our binning data using the [t-Distributed Stochastic Neighbor Embedding (t-SNE)](https://lvdmaaten.github.io/tsne/) algorithm to project high-dimensional data down into a 2D plot that preserves clustering information. There's a really good video on [YouTube](https://www.youtube.com/watch?v=NEaUSP4YerM) that explains how the algorithm works in high-level terms, but for our purposes you can really consider it as a similar approach to a PCA or NMDS.

On its own, `VizBin` takes a set of contigs and performs the *t-SNE* projection using compositional data. We can optionally provide it files that annotate contigs as belonging to particular bins and a file that adds coverage data to be considered when clustering. Unfortuantely, at this stage `VizBin` only allows a single coverage value per contig, which is not ideal. This is because `VizBin` only uses coverage as a means to modify the visualisation, not the ordination itself. It is possible to create your own *t-SNE* projection using multiple coverage values, however this is beyond the scope of today's exercise, and here we will be providing `VizBin` with coverage values for sample1 only. 

The only required input file for `VizBin` is a single `.fna` file of the concatenated bins. An additional annotation file containing per-contig coverage values and bin IDs can also be provided. Colouring contigs by bin is a really effective way to spot areas that might need refinement.


# Update in progress (MH)
##### Add comment re: fragmentation of contigs
...


In the interests of time today, the input files have been generated and are provided in the `6.bin_refinement/` folder: 

* `all_bins.fna` is a concatenation of the bins of *fragmented* sub-contigs (fragmented to 20k)
* `all_bins.sample1.vizbin.ann` is the annotation file containing per-subcontig coverage, label (bin ID), and length values.

For future reference, and for working with your own data, a step-by-step process for generating these files from the output bins generated by `DAS_Tool` has been provided as an appendix at the end of this page.

Let's first have a quick look at the annotation file. 

```bash
head -n5 all_bins.sample1.vizbin.ann
# coverage,label,length
# 17.6361,bin_0.chopped,20000
# 16.2822,bin_0.chopped,20000
# 17.7862,bin_0.chopped,20000
# 16.8073,bin_0.chopped,20000
```

This file is a comma-delimited table (csv file) that presents the information in the way that VizBin expects it. The order of rows in this file corresponds to the order of contigs in the concatentated *fastA* file of our fragmented bins, `all_bins.fna`.

Create a few variations of the *.ann* file with various columns removed, in order to examine the different outputs they can generate.

```bash
cd /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/6.bin_refinement/

# Make a few different versions of the .ann file with various columns removed
## annotation with bin only (to colour by bin)
cut -f2 -d ',' all_bins.sample1.vizbin.ann > all_bins.sample1.vizbin.bin_only.ann
## no length, but coverage and bin included
cut -f1,2 -d ',' all_bins.sample1.vizbin.ann > all_bins.sample1.vizbin.no_length.ann
```

---

### Project a *t-SNE* and examine bin clusters

We can now use these files in `VizBin` to curate the contigs in our bins. We will load and view the data in a few different steps.

Running `VizBin` remotely (e.g. within NeSI) can be slow with full data sets. Running a GUI (such as a program like `VizBin`) remotely can also require additional set up on some PCs. For day-to-day work, we recommend installing `VizBin` on your local machine and downloading the relevant input files (e.g. via `scp ...`) to run locally. 

For today's exercise, open `VizBin` via: 

```bash
cd /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/6.bin_refinement/

java -jar ../tools/vizbin.jar
```

If this fails to open on your PC, or if it runs prohibitively slowly, team up with 2-3 others in the workshop to run through this exercise together on one machine.

#### Loading the input files

Once `VizBin` is open, to get started, simply click the 'Choose...' button then navigate to the *fastA* file.

![](https://github.com/GenomicsAotearoa/metagenomics_summer_school/blob/master/materials/figures/ex10_load_fasta.PNG)

Once this is imported, use the 'Show additional options' button to expose the advanced options, and add your **'bin only'** *.ann* file into the 'Annotation file (optional)' field.

![](https://github.com/GenomicsAotearoa/metagenomics_summer_school/blob/master/materials/figures/ex10_load_ann.PNG)

#### Executing the *t-SNE*

For now leave all other parameters as default. Click the 'Start' button to begin building the ordination. When it completes, you should see an output similar to the following:

##### Contigs coloured by bin

![](https://github.com/GenomicsAotearoa/metagenomics_summer_school/blob/master/materials/figures/ex10_bin_only.PNG)

##### Contigs coloured by bin, sized by length, shaded by coverage

![](https://github.com/GenomicsAotearoa/metagenomics_summer_school/blob/master/materials/figures/ex10_all_ann.PNG)

Similar to any other projection technique, we interpret the closeness of points as a proxy for how similar they are, and because of our *.ann* file we can see which contigs belong to the same bin.

---

### Picking refined bins

We can use the interactive GUI to pick the boundaries of new bins, or to identify contigs which we do not believe should be retained in the data. Have a play around with the interface, testing out the following commands:

1. Left-click and drag: Highlight an area of the ordination to zoom into
1. Right-click, 'Zoom Out', 'Both Axes': Rest of the view
1. Left-click several points: Create a selection of contigs to extract from the data
1. Right-click, 'Selection', 'Export': Save the selected contigs into a new file
1. Right-click, 'Selection', 'Clear selection': Clear the current selection

How you proceed in this stage is up to you. You can either select bins based on their boundary, and call these the refined bins. Alternatively, you could select outlier contigs and examine these in more detail to determine whether or not they were correctly placed into the bin. Which way you proceed really depends on how well the ordination resolves your bins, and it might be that both approaches are needed.

Today, we will run through an example of selecting potentially problematic (sub)contigs, and then deciding whether or not we want to filter these contigs out of our refined bins. We will use a combination of `VizBin` and `seqmagick` to remove contigs from bins where we do not trust the placement of the contig. We are aiming to reduce each bin to a trusted set of contigs.

##### Export `VizBin` clusters

First, for each `VizBin` cluster, select the area around the cluster (via multiple left-clicks around the cluster), right-click, 'Selection', 'Export'. Save this output as `cluster_1.fna`. 

Try this for one or two clusters. In practice, we would do this for each `VizBin` cluster, saving each as a new `cluster_n.fna` file.

...

# Update in progress... (MH)

...


##### Highlight an area with problematic contigs to zoom into

![](https://github.com/GenomicsAotearoa/metagenomics_summer_school/blob/master/materials/figures/ex10_select_to_zoom.PNG)

##### Select the contigs to examine

Left-click several points around a selection of potentially problematic contigs

![](https://github.com/GenomicsAotearoa/metagenomics_summer_school/blob/master/materials/figures/ex10_select_outlier.PNG)

##### Export the contigs

Right-click, 'Selection', 'Export'. 

Save the output as `contigs_1.fna`. 

![](https://github.com/GenomicsAotearoa/metagenomics_summer_school/blob/master/materials/figures/ex10_export.PNG)

Try this for one or two problematic contigs (or subsets of problematic contigs). In practice, you could repeat this for all potential problemtic contigs, saving each export as a new `contigs_n.fna` file.

*NOTE: for the subsequent step using `vizbin_count_table.sh`, all exported cluster files must share a common prefix (e.g. `cluster...fna`), and all files of problematic contigs must also share a common prefix (e.g. `contigs...fna`).*


##### Create a count table of counts of our problematic contigs across each bin

You'll recall that, prior running `VizBin`, we first cut contigs in our bins into 20k fragments to improve the density of the clusters in the *t-SNE* projection. As such, the problematic contigs we have exported from `VizBin` are *sub-contig* fragments, rather than full contigs from our bins. It is entirely possible that different fragments of the original contigs have been placed in different clusters during our `VizBin` analysis - including cases where most sub-contigs have clustered with the bin we expect, and a small number have been identified as "problematic" (i.e. clustered with other bins). Based on the information from these extracted problematic sub-contigs, we now have to carefully consider whether or not we want to remove the *full* contig from our bin data.

To do this, we will generate a table containing each exported "problematic" sub-contig, and counts of how many of its sister sub-contigs (each of the other sub-contig fragments derived from the same original parent contig) fall into each `VizBin` cluster.

For this exercise, a folder of the exported files from `VizBin` for all clusters (`cluster_[1-n].fna`) and problematic sub-contigs (`contigs_[1-n].fna`) has been provided at `vizbin_example_exports/`

We will input these files to the shell script `vizbin_count_table.sh` to generate a count table of the exprted subcontigs across each `VizBin` cluster (`vb_count_table.txt`), as well as a working list of contigs to potentially remove from our final bin data (`vb_omit_contigs_tmp.txt`).

```bash
./vizbin_count_table.sh -i vizbin_example_exports/
```

The only required input to `vizbin_count_table.sh` is the path to the cluster and contigs files exported from `VizBin`. By default, the script looks for the prefix `cluster...` for the cluster file names, `contig...` for the files of problematic sub-contigs, and the file extension `.fna` for each. The arguments `-s <contig_file_prefix> -c <cluster_file_prefix> -e <fasta_file_extension>` can optionally be provided if your file name formats differ from the default.

View the output count table:

```bash
less vb_count_table.txt
```

Example excerpt:

| Subcontig_ID | Subcontig_vb_cluster | cluster_10_count | cluster_1_count | cluster_2_count | cluster_3_count | cluster_4_count | cluster_5_count | cluster_6_count | cluster_7_count | cluster_8_count | cluster_9_count | Total_count |
| :- | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: |
| >bin_5_NODE_826_length_1788_cov_0.154169 |cluster_5   |    0     |  0    |   0    |   0   |    0   |    1   |    0     |  0   |    0     |  0    |   1 |
| >bin_5_NODE_848_length_1686_cov_0.184026   |     cluster_5    |   0    |   0    |   0   |   0    |   0 |     1    |   0    |   0     |  0     |  0    |   1  |
| >bin_9_NODE_4_length_793571_cov_0.517196.33  |   cluster_6   |    0    |   0    |   0    |   0    |   0  |     0   |    1    |   38  |    0   |    0    |   39 |


##### Generate a list of contigs to *exclude* from filtering

Create a list of contigs identified from vb_count_table.txt that are *not* to be filtered out by seqmagick in the next step. For example, those contigs that have sub-contigs split across multiple vizbin clusters, and for which it's reasonable to actually keep the contig (such as when a flagged selected sub-contig exported from vizbin is in one unexpected cluster, but all other sub-contigs from that parent contig are in the expected cluster; in this case, you likely *don't* want to filter out the parent contig from the data set moving forward). 

Below is an example. Simply replace the contig IDs between the quotes for as many lines as necessary for your data. 

*NOTES:*

1. *The first line below must always have only one `>` character, while all subsequent lines must have two (i.e. `>>`) to append correctly to the list.*
2. *We want the original contig ID here, *not* the sub-contig, so make sure to remove the `.xx` fragment number at the end if there is one.*

```bash
echo "bin_0_NODE_9_length_392609_cov_1.038712" > vb_keep_contigs.txt
echo "bin_9_NODE_4_length_793571_cov_0.517196" >> vb_keep_contigs.txt
echo "bin_1_NODE_182_length_42779_cov_1.585353" >> vb_keep_contigs.txt
```

##### Create final `vb_omit_contigs_filtered.txt` list of contigs to filter from bins

Using **grep**, filter contigs we wish to keep (after assessing `vb_count_table.txt`) out of the working `vb_omit_contigs_tmp.txt` list. 

This creates `vb_omit_contigs_filtered.txt`, which we will then pass to **seqmagick** to filter these contigs out of our actual bin fasta files.

```bash
grep -v -f vb_keep_contigs.txt vb_omit_contigs_tmp.txt > vb_omit_contigs_filtered.txt
```

##### Filter suspect contigs (based on `VizBin` analysis) from the bin data

Use `seqmagick --exclude-from-file ...` to filter problematic contigs (those contigs listed in `vb_omit_contigs_filtered.txt`) out of the initial *unchopped* bin fasta files, generating final bins for downstream processing.

```bash 
mkdir filtered_bins/

# Load seqmagick
module load seqmagick/0.7.0-gimkl-2018b-Python-3.7.3

# filter problematic contigs out of original bin files
for bin_file in example_data_unchopped/*.fna; do
    bin_name=$(basename ${bin_file} .fna)
    seqmagick convert --exclude-from-file vb_omit_contigs_filtered.txt ${bin_file} filtered_bins/${bin_name}.filtered.fna
done
```

Our filtered bins for downstream use are now in `filtered_bins/`

---

### *Optional:* Creating new *VizBin* profiles with different fragment lengths

The data you have been working with was created using the `cut_up_fasta.py` script that comes with the binning tool `CONCOCT`. It was run to cut contigs into 20k fragments, to better add density to the cluster. If you would like to visualise the data using different contig fragment sizes, you can create these using the following commands:

```bash
module load CONCOCT/1.0.0-gimkl-2018b-Python-2.7.16

mkdir custom_chop/

for bin_file in example_data_unchopped/*;
do
    bin_name=$(basename ${bin_file} .fna)
    cut_up_fasta.py -c YOUR_CONTIG_SIZE -o 0 --merge_last ${bin_file} > custom_chop/${bin_name}.chopped.fna
done
```

~~You can then create an *.ann* file using the same `python` script as above, but we will not be able to add in the coverage information for this run. Because the `build_vizbin_inputs.py` script is written in version 3 of `python`, but the `CONCOCT` module loads version 2, we need to reload `python 3.7` before running the script again.~~

~~```bash
module load Python/3.7.3-gimkl-2018b
python build_vizbin_inputs.py -o vb_sample1 custom_chop/*
```~~

...

# Update in progress... (MH)

...

---

### *Optional:* Scripts for processing data with *ESOMana*

A suite of tools for creating input files for `ESOMana` can be found on github [here](https://github.com/tetramerFreqs/Binning).

The tool `ESOMana` can be downloaded from [SourceForge](http://databionic-esom.sourceforge.net/).

---

### Assigning taxonomy to the refined bins

It is always valuable to know the taxonomy of our binned MAGs, so that we can link them to the wider scientific literature. In order to do this, there are a few different options available to us:

1. Extract 16S rRNA gene sequences from the MAGs and classify them
1. Annotate each gene in the MAG and take the consensus taxonomy
1. Use a profiling tool like `Kraken`, which matches pieces of DNA to a reference database using *k*-mer searches
1. Identify a core set of genes in the MAG, and use these to compute a species phylogeny

For this exercise, we will use the last option in the list, making use of the `GTDB-TK` software (available on [github](https://github.com/Ecogenomics/GTDBTk)) to automatically identify a set of highly conserved, single copy marker genes which are diagnostic of the bacterial (120 markers) and archaeal (122 markers) lineages. Briefly, `GTDB-TK` will perform the following steps on a set of bins.

1. Attempt to identify a set of 120 bacterial marker genes, and 122 archaeal marker genes in each MAG.
1. Based on the recovered numbers, identify which domain is a more likely assignment for each MAG
1. Create a concatenated alignment of the domain-specific marker genes, spanning approximately 41,000 amino acid positions
1. Filter the alignment down to approximately 5,000 informative sites
1. Insert each MAG into a reference tree create from type material and published MAGs
1. Scale the branch lengths of the resulting tree, as described in [Parks et al.](https://www.ncbi.nlm.nih.gov/pubmed/30148503), to identify an appropriate rank to each branch event in the tree
1. Calculate ANI and AAI statistics between each MAG and its nearest neighbours in the tree
1. Report the resulting taxonomic assignment, and gene alignment

This can all be achieved in a single command, although it must be performed through a slurm script due to the high memory requirements of the process.

```bash
#!/bin/bash
#SBATCH -A nesi02659
#SBATCH -J gtdbtk_test
#SBATCH --partition ga_bigmem
#SBATCH --res SummerSchool
#SBATCH --time 2:00:00
#SBATCH --mem 120GB
#SBATCH --cpus-per-task 10
#SBATCH -e gtdbtk_test.err
#SBATCH -o gtdbtk_test.out

module load GTDB-Tk/0.2.2-gimkl-2018b-Python-2.7.16

cd /nesi/nobackup/nesi02659/MGSS_U/<YOUR FOLDER>/6.bin_refinement/

gtdbtk classify_wf -x fna --cpus 10 --genome_dir filtered_bins/ --out_dir gtdbtk_out/
```

As usual, lets look at the parameters here

|Parameter|Function|
|:---|:---|
|**classify_wf**|Specifies the sub-workflow from `GTDB-TK` that we wish to use|
|**-x ...**|Specify the file extension for MAGs within our input directory.<br>Default is *.fna*, but it's always good practice to specify it anyway|
|**--cpus ...**|Number of CPUs to use when finding marker genes, and performing tree insertion operations|
|**--genome_dir ...**|Input directory containing MAGs as individual *fastA* files|
|**--out_dir ...**|Output directory to write the final set of files|

Before submitting your job, think carefully about which set of MAGs you want to classify. You could either use the raw `DAS_Tool` outputs in the `dastool_out/_DASTool_bins/` folder, the renamed set of bins in the `example_data_unchopped/` folder, the set of curated bins in the `filtered_bins/` folder, or your own set of refined bins. Whichever set you choose, make sure you select the correct input folder and extension setting as it may differ from the example here.

When the task completes, you will have a number of output files provided. The main ones to look for are `gtdbtk.bac120.summary.tsv` and `gtdbtk.arch122.summary.tsv` which report the taoxnomies for your MAGs, split at the domain level. These file are only written if MAGs that fall into the domain were found in your data set, so for this exercise we do not expect to see the `gtdbtk.arch122.summary.tsv` file.

If you are interested in performing more detailed phylogenetic analysis of the data, the filtered multiple sequence alignment (MSA) for the data are provided in the `gtdbtk.bac120.msa.fasta` and `gtdbtk.arch122.msa.fasta` files.

Have a look at your resulting taxonomy. The classification of your MAGs will be informative when addressing your research goal for this workshop.

---

### Appendix: Generating input files for `VizBin` from `DAS_Tool` curated bins

...



...

# Update in progress... (MH)

...

##### Step 2

The `build_vizbin_inputs.py` script is what produces our input files for `VizBin`. It takes the folder of bin files as the input, and optionally a table of sub-contig coverage values, which is defined by the `-c` parameter.

What this script is doing is taking each fasta file and picking out the names of the contigs found in that file (bin). It is then looking for any coverage information which is associated with that contig in the `sample1.txt` file, and adding that as a new column to the file. The resulting information is written to a file that carries the name from the `-o` parameter. 




